{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "DE = data.Field(tokenize=tokenize_de,\n",
    "                init_token='<SOS>',\n",
    "                eos_token='<EOS>',\n",
    "                fix_length=20,\n",
    "                lower=True,\n",
    "                batch_first=True)\n",
    "EN = data.Field(tokenize=tokenize_en,\n",
    "                init_token='<SOS>',\n",
    "                eos_token='<EOS>',\n",
    "                lower=True,\n",
    "                fix_length=20,\n",
    "                batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = datasets.TranslationDataset(path='./data/train', \n",
    "                                    exts=('.de.txt', '.en.txt'),\n",
    "                                    fields=(DE, EN))\n",
    "\n",
    "test = datasets.TranslationDataset(path='./data/test', \n",
    "                                   exts=('.de.txt', '.en.txt'),\n",
    "                                   fields=(DE, EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DE.build_vocab(train.src, min_freq=3)\n",
    "EN.build_vocab(train, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_iter = data.BucketIterator(dataset=test, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Transformer(len(DE.vocab),len(EN.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model-50000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (src_emb): Input_Embedding(\n",
       "    (word_emb): Embedding(39112, 512, padding_idx=2)\n",
       "    (position_emb): Embedding(20, 512)\n",
       "  )\n",
       "  (trg_emb): Input_Embedding(\n",
       "    (word_emb): Embedding(50004, 512, padding_idx=2)\n",
       "    (position_emb): Embedding(20, 512)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (model): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): EncoderBlock(\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (model): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (sf_attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (sf_attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (sf_attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (sf_attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (sf_attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): DecoderBlock(\n",
       "        (sf_attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention): MultiAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (normalize): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): Linear(in_features=512, out_features=50004, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(batch):\n",
    "    src = batch.src\n",
    "    src_mask = 1-(src==1)\n",
    "    src_mask.unsqueeze_(2)\n",
    "    src_position = torch.LongTensor([range(src.size(1))]*src.size(0)).to(src.device)\n",
    "    \n",
    "    trg = batch.trg\n",
    "    trg_mask = 1-(trg==1)\n",
    "    trg_mask.unsqueeze_(2)\n",
    "    trg_position = torch.LongTensor([range(trg.size(1))]*trg.size(0)).to(trg.device)\n",
    "    return src,src_mask,src_position,trg,trg_mask,trg_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch = next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src,src_mask,src_position,trg,trg_mask,trg_position = make_batch(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trg_ = torch.zeros_like(src)\n",
    "trg_[:,0] = 2\n",
    "trg_mask_ = torch.zeros_like(src_mask)\n",
    "trg_mask_[:,0,:] = 1\n",
    "trg_position_ = src_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = model.inference(src,src_mask,src_position.cuda(),trg_,trg_mask_,trg_position_.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"<SOS> it 's the symbol of all that we are and all that we can be as an astonishingly <EOS>\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([EN.vocab.itos[i] for i in trg[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"<SOS> it 's the symbol of all of what we are and what we 're an amazing network for <EOS>\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([EN.vocab.itos[i] for i in output[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
